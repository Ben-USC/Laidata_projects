{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Unsupervised Learning Project.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"Xp3LTq59E8UK","colab_type":"text"},"cell_type":"markdown","source":["# Document Clustering and Topic Modeling"]},{"metadata":{"id":"mS-ob0Q8E8UM","colab_type":"text"},"cell_type":"markdown","source":["*In* this project, we use unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures."]},{"metadata":{"id":"ZE7Qr-ccE8UN","colab_type":"text"},"cell_type":"markdown","source":["## Contents"]},{"metadata":{"id":"2S0waFW9E8UO","colab_type":"text"},"cell_type":"markdown","source":["<ul>\n","<li>[Part 1: Load Data](#Part-1:-Load-Data)\n","<li>[Part 2: Tokenizing and Stemming](#Part-2:-Tokenizing-and-Stemming)\n","<li>[Part 3: TF-IDF](#Part-3:-TF-IDF)\n","<li>[Part 4: K-means clustering](#Part-4:-K-means-clustering)\n","<li>[Part 5: Topic Modeling - Latent Dirichlet Allocation](#Part-5:-Topic-Modeling---Latent-Dirichlet-Allocation)\n","</ul>"]},{"metadata":{"id":"5nXGAelMjJFq","colab_type":"text"},"cell_type":"markdown","source":["# Part 0: Setup Google Drive Environment"]},{"metadata":{"id":"6eT1n7oijJ8v","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0N2bb9S6jKGh","colab_type":"code","colab":{}},"cell_type":"code","source":["file1 = drive.CreateFile({'id':'1t7YL4WDIZcoFpo4o_i0UBf-znpCx-3s_'}) # replace the id with id of file you want to access\n","file1.GetContentFile('synopses_list_imdb.txt')  \n","file2 = drive.CreateFile({'id':'1Wf6hzJSuUfhjUMBZQyARUFms9w1zJZLD'}) # replace the id with id of file you want to access\n","file2.GetContentFile('synopses_list_wiki.txt')  \n","file3 = drive.CreateFile({'id':'1UzUyYzoIt7G_02163v3Fa8KTb-T_bLPT'}) # replace the id with id of file you want to access\n","file3.GetContentFile('title_list.txt')  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Nc9DK62BE8UP","colab_type":"text"},"cell_type":"markdown","source":["# Part 1: Load Data"]},{"metadata":{"id":"OjdBV8gGE8UQ","colab_type":"code","outputId":"beec5da5-c4e7-4065-83a5-fe9665d13df9","executionInfo":{"status":"ok","timestamp":1552771527088,"user_tz":420,"elapsed":348,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","import numpy as np\n","import pandas as pd\n","import nltk\n","# REGULAR EXPRESSION\n","import re\n","import os\n","\n","from sklearn import decomposition\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import matplotlib.pyplot as plt\n","\n","nltk.download('punkt')\n","#nltk.download('stopwords')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"gVsLtiVfE8UU","colab_type":"text"},"cell_type":"markdown","source":["Read data from files. In summary, we have 100 titles and 100 synoposes (combined from imdb and wiki)."]},{"metadata":{"id":"wiGJ0f_gE8UV","colab_type":"code","colab":{}},"cell_type":"code","source":["#import three lists: titles and wikipedia synopses\n","titles = open('title_list.txt').read().split('\\n')\n","titles = titles[:100] #ensures that only the first 100 are read in\n","\n","#The wiki synopses and imdb synopses of each movie is seperated by the keywords \"BREAKS HERE\". \n","#Each synoposes may consist of multiple paragraphs.\n","synopses_wiki = open('synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n","synopses_wiki = synopses_wiki[:100]\n","\n","synopses_imdb = open('synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n","synopses_imdb = synopses_imdb[:100]\n","\n","#Combine imdb and wiki to get full synoposes for the top 100 movies. \n","synopses = []\n","for i in range(len(synopses_wiki)):\n","    item = synopses_wiki[i] + synopses_imdb[i]\n","    synopses.append(item)\n","    \n","#Because these synopses have already been ordered in popularity order, \n","#we just need to generate a list of ordered numbers for future usage.\n","ranks = range(len(titles))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iQ4KGnVeE8UX","colab_type":"text"},"cell_type":"markdown","source":["# Part 2: Tokenizing and Stemming"]},{"metadata":{"collapsed":true,"id":"GHHIaFATE8UY","colab_type":"text"},"cell_type":"markdown","source":["Load stopwords and stemmer function from NLTK library.\n","Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning.\n","Stemming is the process of breaking a word down into its root."]},{"metadata":{"id":"3gSwiUBRE8UY","colab_type":"code","outputId":"7786ec1e-1b1c-4ede-a95e-880084e0ae11","executionInfo":{"status":"ok","timestamp":1552770992326,"user_tz":420,"elapsed":11182,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# Use nltk's English stopwords.\n","stopwords = nltk.corpus.stopwords.words('english')\n","\n","print (\"We use \" + str(len(stopwords)) + \" stop-words from nltk library.\")\n","print (stopwords[:10])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["We use 179 stop-words from nltk library.\n","[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\"]\n"],"name":"stdout"}]},{"metadata":{"id":"e50130X8E8Uc","colab_type":"code","colab":{}},"cell_type":"code","source":["from nltk.stem.snowball import SnowballStemmer\n","stemmer = SnowballStemmer(\"english\")\n","\n","# tokenization and stemming\n","def tokenization_and_stemming(text):\n","    # exclude stop words and tokenize the document, generate a list of string \n","    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n","\n","    filtered_tokens = []\n","    \n","    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n","    for token in tokens:\n","        if re.search('[a-zA-Z]', token):\n","            filtered_tokens.append(token)\n","            \n","    # stemming\n","    stems = [stemmer.stem(t) for t in filtered_tokens]\n","    return stems\n","\n","# tokenization without stemming\n","def tokenization(text):\n","    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n","    filtered_tokens = []\n","    for token in tokens:\n","        if re.search('[a-zA-Z]', token):\n","            filtered_tokens.append(token)\n","    return filtered_tokens"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BwbA6hETE8Uf","colab_type":"code","outputId":"1172af4c-d57d-4fbb-c407-bbf51e24207b","executionInfo":{"status":"error","timestamp":1552771536486,"user_tz":420,"elapsed":456,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":283}},"cell_type":"code","source":["# tokenization and stemming\n","tokenization_and_stemming(\"she looked at her father's arm.\")"],"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-8-30191b3fd161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenization_and_stemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"she looked at her father's arm.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-3107aa60a9ec>\u001b[0m in \u001b[0;36mtokenization_and_stemming\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenization_and_stemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# exclude stop words and tokenize the document, generate a list of string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: global name 'stopwords' is not defined"]}]},{"metadata":{"id":"WtDXMCeME8Uh","colab_type":"text"},"cell_type":"markdown","source":["Use our defined functions to analyze (i.e. tokenize, stem) our synoposes."]},{"metadata":{"id":"CNtXZ3RlE8Ui","colab_type":"code","colab":{}},"cell_type":"code","source":["# 1. do tokenization and stemming for all the documents\n","# 2. also just do tokenization for all the documents\n","# the goal is to create a mapping from stemmed words to original tokenized words for result interpretation.\n","docs_stemmed = []\n","docs_tokenized = []\n","for i in synopses:\n","    tokenized_and_stemmed_results = tokenization_and_stemming(i)\n","    docs_stemmed.extend(tokenized_and_stemmed_results)\n","    \n","    tokenized_results = tokenization(i)\n","    docs_tokenized.extend(tokenized_results)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Kj7JZxnpE8Uk","colab_type":"text"},"cell_type":"markdown","source":["Create a mapping from stemmed words to original tokenized words for result interpretation."]},{"metadata":{"id":"uoTp63fME8Ul","colab_type":"code","outputId":"50ce2011-514b-4854-c020-640fe6b5b891","executionInfo":{"status":"ok","timestamp":1552771015466,"user_tz":420,"elapsed":34274,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["vocab_frame_dict = {docs_stemmed[x]:docs_tokenized[x] for x in range(len(docs_stemmed))}\n","print (vocab_frame_dict['angel'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["angeles\n"],"name":"stdout"}]},{"metadata":{"collapsed":true,"id":"QAWdFqL5E8Uo","colab_type":"text"},"cell_type":"markdown","source":["# Part 3: TF-IDF\n","\n","TF: Term Frequency\n","\n","IDF: Inverse Document Frequency\n","\n","***example:***\n","\n","document1: \"Arthur da Jason\"\n","\n","document 2: \"Jason da da huang\"\n","\n","document1: tf-idf [2, 0.5, 0.5, 0];  document2: tf-idf [0, 1, 0.5, 1]  \n","\n","2-gram: \n","\n","document 1: Arthur da, da Jason; document 2: Jason da, da da, da huang bigram\n","\n","3-gram:\n","\n","document 1: Athur da Jason;  document 2: Jason da da, da da huang"]},{"metadata":{"id":"k-XH7R4pE8Up","colab_type":"code","outputId":"854cd09b-26e5-477b-efa5-697ab6696f0d","executionInfo":{"status":"ok","timestamp":1552771025155,"user_tz":420,"elapsed":43940,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# define vectorizer parameters\n","# TfidfVectorizer will help us to create tf-idf matrix\n","# max_df : maximum document frequency for the given word\n","# min_df : minimum document frequency for the given word\n","# max_features: maximum number of words\n","# use_idf: if not true, we only calculate tf\n","# stop_words : built-in stop words\n","# tokenizer: how to tokenize the document\n","# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram\n","tfidf_model = TfidfVectorizer(max_df=0.8, max_features=2000,\n","                                 min_df=0, stop_words='english',\n","                                 use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,3))\n","\n","tfidf_matrix = tfidf_model.fit_transform(synopses) #fit the vectorizer to synopses\n","\n","print (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n","      \" synoposes and \" + str(tfidf_matrix.shape[1]) + \" terms.\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["In total, there are 100 synoposes and 2000 terms.\n"],"name":"stdout"}]},{"metadata":{"id":"GoRH6IDVE8Ur","colab_type":"code","outputId":"7fc134c6-cbcf-4a6e-e1f1-9c0fd6d5dd62","executionInfo":{"status":"ok","timestamp":1552771025160,"user_tz":420,"elapsed":43930,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"cell_type":"code","source":["# check the parameters\n","tfidf_model.get_params()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'analyzer': u'word',\n"," 'binary': False,\n"," 'decode_error': u'strict',\n"," 'dtype': numpy.float64,\n"," 'encoding': u'utf-8',\n"," 'input': u'content',\n"," 'lowercase': True,\n"," 'max_df': 0.8,\n"," 'max_features': 2000,\n"," 'min_df': 0,\n"," 'ngram_range': (1, 3),\n"," 'norm': u'l2',\n"," 'preprocessor': None,\n"," 'smooth_idf': True,\n"," 'stop_words': 'english',\n"," 'strip_accents': None,\n"," 'sublinear_tf': False,\n"," 'token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n"," 'tokenizer': <function __main__.tokenization_and_stemming>,\n"," 'use_idf': True,\n"," 'vocabulary': None}"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"whu1pCOiE8Uv","colab_type":"text"},"cell_type":"markdown","source":["Save the terms identified by TF-IDF."]},{"metadata":{"id":"lLdKk6n-E8Uw","colab_type":"code","colab":{}},"cell_type":"code","source":["# words\n","tf_selected_words = tfidf_model.get_feature_names()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6mWvzNWQFB26","colab_type":"code","colab":{}},"cell_type":"code","source":["# print out words\n","# tf_selected_words"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k6JFh-1BE8Uy","colab_type":"code","outputId":"ea95986b-2031-440a-d05b-e6bd2e6c1a33","executionInfo":{"status":"ok","timestamp":1552771025166,"user_tz":420,"elapsed":43882,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# tf-idf matrix\n","tfidf_matrix"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<100x2000 sparse matrix of type '<type 'numpy.float64'>'\n","\twith 31769 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"1hPhgwSeE8U4","colab_type":"text"},"cell_type":"markdown","source":["# (Optional) Calculate Document Similarity"]},{"metadata":{"scrolled":true,"id":"NI1CN6m6E8U5","colab_type":"code","outputId":"ba9be51c-9463-474e-bdeb-0bf471e84f51","executionInfo":{"status":"ok","timestamp":1552771025167,"user_tz":420,"elapsed":43864,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"cell_type":"code","source":["# use cosine similarity to check the similarity for two documents\n","from sklearn.metrics.pairwise import cosine_similarity\n","cos_matrix = cosine_similarity(tfidf_matrix)\n","print (cos_matrix)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1.         0.01587253 0.01952566 ... 0.02251429 0.02280225 0.04400671]\n"," [0.01587253 1.         0.0314611  ... 0.01242858 0.01335847 0.01926311]\n"," [0.01952566 0.0314611  1.         ... 0.01612858 0.01253687 0.04145735]\n"," ...\n"," [0.02251429 0.01242858 0.01612858 ... 1.         0.03184383 0.0459994 ]\n"," [0.02280225 0.01335847 0.01253687 ... 0.03184383 1.         0.01976441]\n"," [0.04400671 0.01926311 0.04145735 ... 0.0459994  0.01976441 1.        ]]\n"],"name":"stdout"}]},{"metadata":{"id":"XEcwtws5E8U8","colab_type":"text"},"cell_type":"markdown","source":["# Part 4: K-means clustering"]},{"metadata":{"scrolled":true,"id":"7LJQ5i3IE8U9","colab_type":"code","colab":{}},"cell_type":"code","source":["# k-means clustering\n","from sklearn.cluster import KMeans\n","\n","# number of clusters\n","num_clusters = 5\n","km = KMeans(n_clusters=num_clusters)\n","km.fit(tfidf_matrix)\n","clusters = km.labels_.tolist()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fboVpRAfE8U-","colab_type":"text"},"cell_type":"markdown","source":["## 4.1. Analyze K-means Result"]},{"metadata":{"id":"KGs4aIIME8U_","colab_type":"code","colab":{}},"cell_type":"code","source":["# create DataFrame films from all of the input files.\n","films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters}\n","frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"APmEUmm6E8VC","colab_type":"code","outputId":"a9129b06-ab81-4f37-c1f7-1e6de18acf7f","executionInfo":{"status":"ok","timestamp":1552771025923,"user_tz":420,"elapsed":44596,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":359}},"cell_type":"code","source":["frame.head(10)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rank</th>\n","      <th>title</th>\n","      <th>cluster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>The Godfather</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>The Shawshank Redemption</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>Schindler's List</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Raging Bull</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>Casablanca</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>One Flew Over the Cuckoo's Nest</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6</td>\n","      <td>Gone with the Wind</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7</td>\n","      <td>Citizen Kane</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>8</td>\n","      <td>The Wizard of Oz</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>9</td>\n","      <td>Titanic</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   rank                            title  cluster\n","3     0                    The Godfather        3\n","0     1         The Shawshank Redemption        0\n","0     2                 Schindler's List        0\n","2     3                      Raging Bull        2\n","2     4                       Casablanca        2\n","4     5  One Flew Over the Cuckoo's Nest        4\n","2     6               Gone with the Wind        2\n","2     7                     Citizen Kane        2\n","4     8                 The Wizard of Oz        4\n","0     9                          Titanic        0"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"Ht1SbbOSE8VE","colab_type":"code","outputId":"fcfda6c3-cdcc-4fe7-f015-1835c08ed850","executionInfo":{"status":"ok","timestamp":1552771025923,"user_tz":420,"elapsed":44583,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"cell_type":"code","source":["print (\"Number of films included in each cluster:\")\n","frame['cluster'].value_counts().to_frame()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of films included in each cluster:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cluster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   cluster\n","2       48\n","0       27\n","1       12\n","3        7\n","4        6"]},"metadata":{"tags":[]},"execution_count":19}]},{"metadata":{"scrolled":false,"id":"PEk3P4BTE8VI","colab_type":"code","outputId":"b8c13f39-9c08-48b4-c3e2-7f92554cac78","executionInfo":{"status":"error","timestamp":1552771026355,"user_tz":420,"elapsed":45000,"user":{"displayName":"Arthur M","photoUrl":"","userId":"16377123886564016720"}},"colab":{"base_uri":"https://localhost:8080/","height":130}},"cell_type":"code","source":["print (\"<Document clustering result by K-means>\")\n","\n","#km.cluster_centers_ denotes the importances of each items in centroid.\n","#We need to sort it in decreasing-order and get the top k items.\n","order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n","\n","Cluster_keywords_summary = {}\n","for i in range(num_clusters):\n","    print (\"Cluster \" + str(i) + \" words:\", end='')\n","    Cluster_keywords_summary[i] = []\n","    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n","        Cluster_keywords_summary[i].append(vocab_frame_dict[tf_selected_words[ind]])\n","        print (vocab_frame_dict[tf_selected_words[ind]] + \",\", end='')\n","    print ()\n","    #Here ix means index, which is the clusterID of each item.\n","    #Without tolist, the values result from dataframe is <type 'numpy.ndarray'>\n","    cluster_movies = frame.ix[i]['title'].values.tolist()\n","    print (\"Cluster \" + str(i) + \" titles (\" + str(len(cluster_movies)) + \" movies): \")\n","    print (\", \".join(cluster_movies))\n","    print ()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-c0e2a36657f2>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    print (\"Cluster \" + str(i) + \" words:\", end='')\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"metadata":{"id":"DdpmzonWE8VM","colab_type":"text"},"cell_type":"markdown","source":["## 4.2. Plot K-means Result"]},{"metadata":{"id":"a66VxmlYE8VM","colab_type":"code","colab":{}},"cell_type":"code","source":["# use pca to reduce dimensions to 2d for visibility, just want to see if there 2d can give us some insights\n","# this is not an appropriate method, just a guess.\n","pca = decomposition.PCA(n_components=2)\n","tfidf_matrix_np=tfidf_matrix.toarray()\n","pca.fit(tfidf_matrix_np)\n","X = pca.transform(tfidf_matrix_np)\n","\n","xs, ys = X[:, 0], X[:, 1]\n","\n","#set up colors per clusters using a dict\n","cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n","#set up cluster names using a dict\n","cluster_names = {}\n","for i in range(num_clusters):\n","    cluster_names[i] = \", \".join(Cluster_keywords_summary[i])"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"829bBRgEE8VP","colab_type":"code","colab":{}},"cell_type":"code","source":["# %matplotlib inline \n","\n","#create data frame with PCA cluster results\n","df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n","groups = df.groupby(clusters)\n","\n","# set up plot\n","fig, ax = plt.subplots(figsize=(16, 9))\n","#Set color for each cluster/group\n","for name, group in groups:\n","    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n","            label=cluster_names[name], color=cluster_colors[name], \n","            mec='none')\n","\n","ax.legend(numpoints=1,loc=4)  #show legend with only 1 point, position is right bottom.\n","\n","plt.show() #show the plot"],"execution_count":0,"outputs":[]},{"metadata":{"id":"InZXGIXhE8VT","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oYOZXL53E8VV","colab_type":"text"},"cell_type":"markdown","source":["# Part 5: Topic Modeling - Latent Dirichlet Allocation"]},{"metadata":{"id":"lBPVbFNFE8VW","colab_type":"code","colab":{}},"cell_type":"code","source":["# Use LDA for clustering\n","from sklearn.decomposition import LatentDirichletAllocation\n","lda = LatentDirichletAllocation(n_components=5, learning_method = 'online')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L5YYz__7E8VY","colab_type":"code","colab":{}},"cell_type":"code","source":["# LDA requires integer values, keep first 3 digits\n","tfidf_matrix_lda = (tfidf_matrix * 100)\n","tfidf_matrix_lda = tfidf_matrix_lda.astype(int)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qIwUNN0ZE8Vb","colab_type":"code","colab":{}},"cell_type":"code","source":["lda.fit(tfidf_matrix_lda)"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"TukTmm7dE8Vd","colab_type":"code","colab":{}},"cell_type":"code","source":["# topics and words matrix\n","topic_word = lda.components_\n","print(topic_word.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4Hs012V-E8Vg","colab_type":"code","colab":{}},"cell_type":"code","source":["n_top_words = 7\n","topic_keywords_list = []\n","for i, topic_dist in enumerate(topic_word):\n","    #Here we select top(n_top_words-1)\n","    lda_topic_words = np.array(tf_selected_words)[np.argsort(topic_dist)][:-n_top_words:-1] \n","    for j in range(len(lda_topic_words)):\n","        lda_topic_words[j] = vocab_frame_dict[lda_topic_words[j]]\n","    topic_keywords_list.append(lda_topic_words.tolist())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BuSg4tbSE8Vh","colab_type":"code","colab":{}},"cell_type":"code","source":["# documents and topics matri\n","doc_topic = lda.transform(tfidf_matrix_lda)\n","print (doc_topic.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"paiC0DU-E8Vj","colab_type":"code","colab":{}},"cell_type":"code","source":["# print out the clusters and topics and titles of the movies\n","topic_doc_dict = {}\n","print (\"<Document clustering result by LDA>\")\n","for i in range(len(doc_topic)):\n","    topicID = doc_topic[i].argmax()\n","    if topicID not in topic_doc_dict:\n","        topic_doc_dict[topicID] = [titles[i]]\n","    else:\n","        topic_doc_dict[topicID].append(titles[i])\n","for i in topic_doc_dict:\n","    print (\"Cluster \" + str(i) + \" words: \" + \", \".join(topic_keywords_list[i]))\n","    print (\"Cluster \" + str(i) + \" titles (\" + str(len(topic_doc_dict[i])) + \" movies): \")\n","    print (', '.join(topic_doc_dict[i]))\n","    print ()"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"NGq6OKlQE8Vl","colab_type":"text"},"cell_type":"markdown","source":["# Appendix: K-means"]},{"metadata":{"id":"2i3f5eEeE8Vm","colab_type":"code","colab":{}},"cell_type":"code","source":["# An example of k-means, no relationship with above nlp case. The data is generated randomly.\n","from sklearn.datasets.samples_generator import make_blobs\n","X, y = make_blobs(n_samples=300, centers=4,\n","                  random_state=0, cluster_std=0.60)\n","plt.scatter(X[:, 0], X[:, 1], s=50);"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"47Wx7q_ME8Vp","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.cluster import KMeans\n","est = KMeans(4)  # 4 clusters\n","est.fit(X)\n","y_kmeans = est.predict(X)\n","plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50);"],"execution_count":0,"outputs":[]}]}