{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xp3LTq59E8UK"
   },
   "source": [
    "# Document Clustering and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mS-ob0Q8E8UM"
   },
   "source": [
    "*In* this project, we use unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZE7Qr-ccE8UN"
   },
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2S0waFW9E8UO"
   },
   "source": [
    "<ul>\n",
    "<li>[Part 1: Load Data](#Part-1:-Load-Data)\n",
    "<li>[Part 2: Tokenizing and Stemming](#Part-2:-Tokenizing-and-Stemming)\n",
    "<li>[Part 3: TF-IDF](#Part-3:-TF-IDF)\n",
    "<li>[Part 4: K-means clustering](#Part-4:-K-means-clustering)\n",
    "<li>[Part 5: Topic Modeling - Latent Dirichlet Allocation](#Part-5:-Topic-Modeling---Latent-Dirichlet-Allocation)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nXGAelMjJFq"
   },
   "source": [
    "# Part 0: Setup Google Drive Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eT1n7oijJ8v"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0N2bb9S6jKGh"
   },
   "outputs": [],
   "source": [
    "file1 = drive.CreateFile({'id':'1t7YL4WDIZcoFpo4o_i0UBf-znpCx-3s_'}) # replace the id with id of file you want to access\n",
    "file1.GetContentFile('synopses_list_imdb.txt')  \n",
    "file2 = drive.CreateFile({'id':'1Wf6hzJSuUfhjUMBZQyARUFms9w1zJZLD'}) # replace the id with id of file you want to access\n",
    "file2.GetContentFile('synopses_list_wiki.txt')  \n",
    "file3 = drive.CreateFile({'id':'1UzUyYzoIt7G_02163v3Fa8KTb-T_bLPT'}) # replace the id with id of file you want to access\n",
    "file3.GetContentFile('title_list.txt')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nc9DK62BE8UP"
   },
   "source": [
    "# Part 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1552771527088,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "OjdBV8gGE8UQ",
    "outputId": "beec5da5-c4e7-4065-83a5-fe9665d13df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# REGULAR EXPRESSION\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVsLtiVfE8UU"
   },
   "source": [
    "Read data from files. In summary, we have 100 titles and 100 synoposes (combined from imdb and wiki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wiGJ0f_gE8UV"
   },
   "outputs": [],
   "source": [
    "#import three lists: titles and wikipedia synopses\n",
    "titles = open('title_list.txt').read().split('\\n')\n",
    "titles = titles[:100] #ensures that only the first 100 are read in\n",
    "\n",
    "#The wiki synopses and imdb synopses of each movie is seperated by the keywords \"BREAKS HERE\". \n",
    "#Each synoposes may consist of multiple paragraphs.\n",
    "synopses_wiki = open('synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_wiki = synopses_wiki[:100]\n",
    "\n",
    "synopses_imdb = open('synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_imdb = synopses_imdb[:100]\n",
    "\n",
    "#Combine imdb and wiki to get full synoposes for the top 100 movies. \n",
    "synopses = []\n",
    "for i in range(len(synopses_wiki)):\n",
    "    item = synopses_wiki[i] + synopses_imdb[i]\n",
    "    synopses.append(item)\n",
    "    \n",
    "#Because these synopses have already been ordered in popularity order, \n",
    "#we just need to generate a list of ordered numbers for future usage.\n",
    "ranks = range(len(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQ4KGnVeE8UX"
   },
   "source": [
    "# Part 2: Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "GHHIaFATE8UY"
   },
   "source": [
    "Load stopwords and stemmer function from NLTK library.\n",
    "Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning.\n",
    "Stemming is the process of breaking a word down into its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11182,
     "status": "ok",
     "timestamp": 1552770992326,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "3gSwiUBRE8UY",
    "outputId": "7786ec1e-1b1c-4ede-a95e-880084e0ae11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use 179 stop-words from nltk library.\n",
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Use nltk's English stopwords.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print (\"We use \" + str(len(stopwords)) + \" stop-words from nltk library.\")\n",
    "print (stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e50130X8E8Uc"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# tokenization and stemming\n",
    "def tokenization_and_stemming(text):\n",
    "    # exclude stop words and tokenize the document, generate a list of string \n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n",
    "\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    # stemming\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "# tokenization without stemming\n",
    "def tokenization(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 456,
     "status": "error",
     "timestamp": 1552771536486,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "BwbA6hETE8Uf",
    "outputId": "1172af4c-d57d-4fbb-c407-bbf51e24207b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-30191b3fd161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenization_and_stemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"she looked at her father's arm.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3107aa60a9ec>\u001b[0m in \u001b[0;36mtokenization_and_stemming\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenization_and_stemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# exclude stop words and tokenize the document, generate a list of string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# tokenization and stemming\n",
    "tokenization_and_stemming(\"she looked at her father's arm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtDXMCeME8Uh"
   },
   "source": [
    "Use our defined functions to analyze (i.e. tokenize, stem) our synoposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNtXZ3RlE8Ui"
   },
   "outputs": [],
   "source": [
    "# 1. do tokenization and stemming for all the documents\n",
    "# 2. also just do tokenization for all the documents\n",
    "# the goal is to create a mapping from stemmed words to original tokenized words for result interpretation.\n",
    "docs_stemmed = []\n",
    "docs_tokenized = []\n",
    "for i in synopses:\n",
    "    tokenized_and_stemmed_results = tokenization_and_stemming(i)\n",
    "    docs_stemmed.extend(tokenized_and_stemmed_results)\n",
    "    \n",
    "    tokenized_results = tokenization(i)\n",
    "    docs_tokenized.extend(tokenized_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kj7JZxnpE8Uk"
   },
   "source": [
    "Create a mapping from stemmed words to original tokenized words for result interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34274,
     "status": "ok",
     "timestamp": 1552771015466,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "uoTp63fME8Ul",
    "outputId": "50ce2011-514b-4854-c020-640fe6b5b891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angeles\n"
     ]
    }
   ],
   "source": [
    "vocab_frame_dict = {docs_stemmed[x]:docs_tokenized[x] for x in range(len(docs_stemmed))}\n",
    "print (vocab_frame_dict['angel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "QAWdFqL5E8Uo"
   },
   "source": [
    "# Part 3: TF-IDF\n",
    "\n",
    "TF: Term Frequency\n",
    "\n",
    "IDF: Inverse Document Frequency\n",
    "\n",
    "***example:***\n",
    "\n",
    "document1: \"Arthur da Jason\"\n",
    "\n",
    "document 2: \"Jason da da huang\"\n",
    "\n",
    "document1: tf-idf [2, 0.5, 0.5, 0];  document2: tf-idf [0, 1, 0.5, 1]  \n",
    "\n",
    "2-gram: \n",
    "\n",
    "document 1: Arthur da, da Jason; document 2: Jason da, da da, da huang bigram\n",
    "\n",
    "3-gram:\n",
    "\n",
    "document 1: Athur da Jason;  document 2: Jason da da, da da huang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43940,
     "status": "ok",
     "timestamp": 1552771025155,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "k-XH7R4pE8Up",
    "outputId": "854cd09b-26e5-477b-efa5-697ab6696f0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 100 synoposes and 2000 terms.\n"
     ]
    }
   ],
   "source": [
    "# define vectorizer parameters\n",
    "# TfidfVectorizer will help us to create tf-idf matrix\n",
    "# max_df : maximum document frequency for the given word\n",
    "# min_df : minimum document frequency for the given word\n",
    "# max_features: maximum number of words\n",
    "# use_idf: if not true, we only calculate tf\n",
    "# stop_words : built-in stop words\n",
    "# tokenizer: how to tokenize the document\n",
    "# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram\n",
    "tfidf_model = TfidfVectorizer(max_df=0.8, max_features=2000,\n",
    "                                 min_df=0, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = tfidf_model.fit_transform(synopses) #fit the vectorizer to synopses\n",
    "\n",
    "print (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n",
    "      \" synoposes and \" + str(tfidf_matrix.shape[1]) + \" terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43930,
     "status": "ok",
     "timestamp": 1552771025160,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "GoRH6IDVE8Ur",
    "outputId": "7fc134c6-cbcf-4a6e-e1f1-9c0fd6d5dd62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': u'word',\n",
       " 'binary': False,\n",
       " 'decode_error': u'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': u'utf-8',\n",
       " 'input': u'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 0.8,\n",
       " 'max_features': 2000,\n",
       " 'min_df': 0,\n",
       " 'ngram_range': (1, 3),\n",
       " 'norm': u'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': 'english',\n",
       " 'strip_accents': None,\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': <function __main__.tokenization_and_stemming>,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the parameters\n",
    "tfidf_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whu1pCOiE8Uv"
   },
   "source": [
    "Save the terms identified by TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLdKk6n-E8Uw"
   },
   "outputs": [],
   "source": [
    "# words\n",
    "tf_selected_words = tfidf_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mWvzNWQFB26"
   },
   "outputs": [],
   "source": [
    "# print out words\n",
    "# tf_selected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43882,
     "status": "ok",
     "timestamp": 1552771025166,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "k6JFh-1BE8Uy",
    "outputId": "ea95986b-2031-440a-d05b-e6bd2e6c1a33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x2000 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 31769 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf matrix\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hPhgwSeE8U4"
   },
   "source": [
    "# (Optional) Calculate Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43864,
     "status": "ok",
     "timestamp": 1552771025167,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "NI1CN6m6E8U5",
    "outputId": "ba9be51c-9463-474e-bdeb-0bf471e84f51",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.01587253 0.01952566 ... 0.02251429 0.02280225 0.04400671]\n",
      " [0.01587253 1.         0.0314611  ... 0.01242858 0.01335847 0.01926311]\n",
      " [0.01952566 0.0314611  1.         ... 0.01612858 0.01253687 0.04145735]\n",
      " ...\n",
      " [0.02251429 0.01242858 0.01612858 ... 1.         0.03184383 0.0459994 ]\n",
      " [0.02280225 0.01335847 0.01253687 ... 0.03184383 1.         0.01976441]\n",
      " [0.04400671 0.01926311 0.04145735 ... 0.0459994  0.01976441 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# use cosine similarity to check the similarity for two documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_matrix = cosine_similarity(tfidf_matrix)\n",
    "print (cos_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEcwtws5E8U8"
   },
   "source": [
    "# Part 4: K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LJQ5i3IE8U9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# number of clusters\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fboVpRAfE8U-"
   },
   "source": [
    "## 4.1. Analyze K-means Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGs4aIIME8U_"
   },
   "outputs": [],
   "source": [
    "# create DataFrame films from all of the input files.\n",
    "films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters}\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44596,
     "status": "ok",
     "timestamp": 1552771025923,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "APmEUmm6E8VC",
    "outputId": "a9129b06-ab81-4f37-c1f7-1e6de18acf7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Raging Bull</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Casablanca</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Gone with the Wind</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>The Wizard of Oz</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                            title  cluster\n",
       "3     0                    The Godfather        3\n",
       "0     1         The Shawshank Redemption        0\n",
       "0     2                 Schindler's List        0\n",
       "2     3                      Raging Bull        2\n",
       "2     4                       Casablanca        2\n",
       "4     5  One Flew Over the Cuckoo's Nest        4\n",
       "2     6               Gone with the Wind        2\n",
       "2     7                     Citizen Kane        2\n",
       "4     8                 The Wizard of Oz        4\n",
       "0     9                          Titanic        0"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44583,
     "status": "ok",
     "timestamp": 1552771025923,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "Ht1SbbOSE8VE",
    "outputId": "fcfda6c3-cdcc-4fe7-f015-1835c08ed850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of films included in each cluster:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster\n",
       "2       48\n",
       "0       27\n",
       "1       12\n",
       "3        7\n",
       "4        6"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Number of films included in each cluster:\")\n",
    "frame['cluster'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45000,
     "status": "error",
     "timestamp": 1552771026355,
     "user": {
      "displayName": "Arthur M",
      "photoUrl": "",
      "userId": "16377123886564016720"
     },
     "user_tz": 420
    },
    "id": "PEk3P4BTE8VI",
    "outputId": "b8c13f39-9c08-48b4-c3e2-7f92554cac78",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-c0e2a36657f2>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    print (\"Cluster \" + str(i) + \" words:\", end='')\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print (\"<Document clustering result by K-means>\")\n",
    "\n",
    "#km.cluster_centers_ denotes the importances of each items in centroid.\n",
    "#We need to sort it in decreasing-order and get the top k items.\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "Cluster_keywords_summary = {}\n",
    "for i in range(num_clusters):\n",
    "    print (\"Cluster \" + str(i) + \" words:\", end='')\n",
    "    Cluster_keywords_summary[i] = []\n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        Cluster_keywords_summary[i].append(vocab_frame_dict[tf_selected_words[ind]])\n",
    "        print (vocab_frame_dict[tf_selected_words[ind]] + \",\", end='')\n",
    "    print ()\n",
    "    #Here ix means index, which is the clusterID of each item.\n",
    "    #Without tolist, the values result from dataframe is <type 'numpy.ndarray'>\n",
    "    cluster_movies = frame.ix[i]['title'].values.tolist()\n",
    "    print (\"Cluster \" + str(i) + \" titles (\" + str(len(cluster_movies)) + \" movies): \")\n",
    "    print (\", \".join(cluster_movies))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdpmzonWE8VM"
   },
   "source": [
    "## 4.2. Plot K-means Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a66VxmlYE8VM"
   },
   "outputs": [],
   "source": [
    "# use pca to reduce dimensions to 2d for visibility, just want to see if there 2d can give us some insights\n",
    "# this is not an appropriate method, just a guess.\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "tfidf_matrix_np=tfidf_matrix.toarray()\n",
    "pca.fit(tfidf_matrix_np)\n",
    "X = pca.transform(tfidf_matrix_np)\n",
    "\n",
    "xs, ys = X[:, 0], X[:, 1]\n",
    "\n",
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {}\n",
    "for i in range(num_clusters):\n",
    "    cluster_names[i] = \", \".join(Cluster_keywords_summary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "829bBRgEE8VP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline \n",
    "\n",
    "#create data frame with PCA cluster results\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "groups = df.groupby(clusters)\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "#Set color for each cluster/group\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "\n",
    "ax.legend(numpoints=1,loc=4)  #show legend with only 1 point, position is right bottom.\n",
    "\n",
    "plt.show() #show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InZXGIXhE8VT"
   },
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYOZXL53E8VV"
   },
   "source": [
    "# Part 5: Topic Modeling - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBPVbFNFE8VW"
   },
   "outputs": [],
   "source": [
    "# Use LDA for clustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=5, learning_method = 'online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5YYz__7E8VY"
   },
   "outputs": [],
   "source": [
    "# LDA requires integer values, keep first 3 digits\n",
    "tfidf_matrix_lda = (tfidf_matrix * 100)\n",
    "tfidf_matrix_lda = tfidf_matrix_lda.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIwUNN0ZE8Vb"
   },
   "outputs": [],
   "source": [
    "lda.fit(tfidf_matrix_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TukTmm7dE8Vd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# topics and words matrix\n",
    "topic_word = lda.components_\n",
    "print(topic_word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Hs012V-E8Vg"
   },
   "outputs": [],
   "source": [
    "n_top_words = 7\n",
    "topic_keywords_list = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    #Here we select top(n_top_words-1)\n",
    "    lda_topic_words = np.array(tf_selected_words)[np.argsort(topic_dist)][:-n_top_words:-1] \n",
    "    for j in range(len(lda_topic_words)):\n",
    "        lda_topic_words[j] = vocab_frame_dict[lda_topic_words[j]]\n",
    "    topic_keywords_list.append(lda_topic_words.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuSg4tbSE8Vh"
   },
   "outputs": [],
   "source": [
    "# documents and topics matri\n",
    "doc_topic = lda.transform(tfidf_matrix_lda)\n",
    "print (doc_topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "paiC0DU-E8Vj"
   },
   "outputs": [],
   "source": [
    "# print out the clusters and topics and titles of the movies\n",
    "topic_doc_dict = {}\n",
    "print (\"<Document clustering result by LDA>\")\n",
    "for i in range(len(doc_topic)):\n",
    "    topicID = doc_topic[i].argmax()\n",
    "    if topicID not in topic_doc_dict:\n",
    "        topic_doc_dict[topicID] = [titles[i]]\n",
    "    else:\n",
    "        topic_doc_dict[topicID].append(titles[i])\n",
    "for i in topic_doc_dict:\n",
    "    print (\"Cluster \" + str(i) + \" words: \" + \", \".join(topic_keywords_list[i]))\n",
    "    print (\"Cluster \" + str(i) + \" titles (\" + str(len(topic_doc_dict[i])) + \" movies): \")\n",
    "    print (', '.join(topic_doc_dict[i]))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "NGq6OKlQE8Vl"
   },
   "source": [
    "# Appendix: K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2i3f5eEeE8Vm"
   },
   "outputs": [],
   "source": [
    "# An example of k-means, no relationship with above nlp case. The data is generated randomly.\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "47Wx7q_ME8Vp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "est = KMeans(4)  # 4 clusters\n",
    "est.fit(X)\n",
    "y_kmeans = est.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50);"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Unsupervised Learning Project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
